const vulnerabilityAssessment = [
  {
    title: "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
    authors: "Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi",
    conference: "Annual Meeting of the Association for Computational Linguistics (ACL), 2024",
    arxiv: "https://arxiv.org/abs/2401.06373",
    highlights: []
  },
  {
    title: "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
    authors: "Xiangyu Qi*, Yi Zeng*, Tinghao Xie*, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson",
    conference: "International Conference on Learning Representations (ICLR), 2024",
    openreview: "https://openreview.net/forum?id=hTEGyKf0dZ&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2024%2FConference%2FAuthors%23your-submissions)",
    highlights: [
      "Oral presentation",
      "Featured in the <a href=\"https://www.nytimes.com/2023/10/19/technology/guardrails-artificial-intelligence-open-source.html\">New York Times</a>"
    ]
  },
  {
    title: "Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information",
    authors: "Yi Zeng*, Minzhou Pan*, Hoang Anh Just, Lingjuan Lyu, Meikang Qiu, Ruoxi Jia",
    conference: "ACM Conference on Computer and Communications Security (CCS), 2023",
    arxiv: "https://arxiv.org/abs/2204.05255",
    highlights: []
  },
  {
    title: "Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective",
    authors: "Yi Zeng*, Won Park*, Z. Morley Mao, Ruoxi Jia",
    conference: "International Conference on Computer Vision (ICCV), 2021",
    arxiv: "https://arxiv.org/abs/2104.03413",
    highlights: []
  }
];

const robustness = [
  {
    title: "Towards Robustness Certification Against Universal Perturbations",
    authors: "Yi Zeng*, Zhouxing Shi*, Ming Jin, Feiyang Kang, Lingjuan Lyu, Cho-Jui Hsieh, Ruoxi Jia",
    conference: "International Conference on Learning Representations (ICLR), 2023",
    openreview: "https://openreview.net/forum?id=7GEvPKxjtt&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2023%2FConference%2FAuthors%23your-submissions)",
    highlights: []
  },
  {
    title: "Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion",
    authors: "Si Chen, Yi Zeng, Won Park, Jiachen T. Wang, Xun Chen, Lingjuan Lyu, Zhuoqing Mao, Ruoxi Jia",
    conference: "Transactions on Machine Learning Research, 2023",
    openreview: "https://openreview.net/forum?id=P880C39xAvM",
    highlights: []
  },
  {
    title: "Adversarial Unlearning of Backdoors via Implicit Hypergradient",
    authors: "Yi Zeng, Si Chen, Won Park, Z. Morley Mao, Ming Jin, Ruoxi Jia",
    conference: "International Conference on Learning Representations (ICLR), 2022",
    arxiv: "https://arxiv.org/abs/2110.03735",
    highlights: []
  },
  {
    title: "Robust anomaly detection and backdoor attack detection via differential privacy",
    authors: "Min Du, Ruoxi Jia, Dawn Song",
    conference: "International Conference on Learning Representations (ICLR), 2020",
    arxiv: "https://arxiv.org/abs/1911.07116",
    highlights: []
  }
];

const alignment = [
  {
    title: "Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs",
    authors: "Bilgehan Sel, Priya Shanmugasundaram, Mohammad Kachuee, Kun Zhou, Ruoxi Jia, Ming Jin",
    conference: "Annual Meeting of the Association for Computational Linguistics (ACL), 2024",
    arxiv: "https://arxiv.org/abs/2405.12933",
    highlights: []
  }
];

export { vulnerabilityAssessment, robustness, alignment };